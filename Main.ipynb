{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd515bd",
   "metadata": {},
   "source": [
    "# List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bb373",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import timm \n",
    "print(timm.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb3550",
   "metadata": {},
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d827a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import rotate\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, RandAugment\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, RandomResizedCrop, ToTensor, Normalize\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d270df",
   "metadata": {},
   "source": [
    "# Loading data and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf89c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        labeled_train_dataset_path,\n",
    "        unlabeled_train_dataset_path,\n",
    "        train_transform,\n",
    "        batch_size,\n",
    "        num_workers,\n",
    "    ):\n",
    "\n",
    "        self.train_transform = train_transform\n",
    "        self.labeled_train_dataset_path = labeled_train_dataset_path\n",
    "        self.unlabeled_train_dataset_path = unlabeled_train_dataset_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "\n",
    "    def labeled_dataloader(self, j):\n",
    "        \n",
    "        transform = Compose([\n",
    "            RandAugment(),\n",
    "            RandomResizedCrop(224, scale=(1.0, 1.0)),\n",
    "            ToTensor()\n",
    "        ])\n",
    "        labeled_dataset = ImageFolder(self.labeled_train_dataset_path, transform=self.train_transform)\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "            labeled_dataset,\n",
    "            [\n",
    "                int(0.8 * len(labeled_dataset)),\n",
    "                len(labeled_dataset) - int(0.8 * len(labeled_dataset)),\n",
    "            ],\n",
    "            generator=torch.Generator().manual_seed(3300 + j),\n",
    "        )\n",
    "        labeled_augmented_dataset =[ ImageFolder(self.labeled_train_dataset_path, transform=transform) for i in range(10)]\n",
    "        augmented_dataset = [ torch.utils.data.random_split(\n",
    "            dataset,\n",
    "            [\n",
    "                int(0.8 * len(dataset)),\n",
    "                len(dataset) - int(0.8 * len(dataset)),\n",
    "            ],\n",
    "            generator=torch.Generator().manual_seed(3300+j),\n",
    "        )[0] for dataset in labeled_augmented_dataset ]\n",
    "\n",
    "        \n",
    "        train_combined_dataset_temp = torch.utils.data.dataset.ConcatDataset([augmented_dataset[i] for i in range (10)])\n",
    "        train_combined_dataset = torch.utils.data.dataset.ConcatDataset([train_combined_dataset_temp,train_dataset ])\n",
    "        traindataloader = DataLoader(\n",
    "            train_combined_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "\n",
    "        valdataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        \n",
    "        return traindataloader, valdataloader\n",
    "\n",
    "    def unlabeled_dataloader(self):\n",
    "        unlabeled_dataset = UnlabeledDataset(self.unlabeled_train_dataset_path, transform=self.train_transform)\n",
    "        unlabeleddataloader =  DataLoader(\n",
    "            unlabeled_dataset,\n",
    "            batch_size=28,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "        )\n",
    "        \n",
    "        return unlabeleddataloader\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataset_path, transform=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.data = self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        data = []\n",
    "        image_files = os.listdir(self.dataset_path)\n",
    "        n = 0\n",
    "        for file_name in image_files:\n",
    "            file_path = os.path.join(self.dataset_path, file_name)\n",
    "            image = Image.open(file_path).convert(\"RGB\")\n",
    "            data.append(image)\n",
    "            if n >60000:\n",
    "                return data\n",
    "            n +=1\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, -1  # Use a pseudo-label of -1 for unlabeled samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def blend_images(image1, image2, f):\n",
    "    # Ensure the images have the same shape\n",
    "    assert image1.shape == image2.shape, \"Images must have the same shape\"\n",
    "\n",
    "    # Perform the image blending\n",
    "    blended_image = (1 - f) * image1 + f * image2\n",
    "    \n",
    "    return blended_image\n",
    "\n",
    "def blend_labels(label1,label2,f,max_l):\n",
    "    label = np.zeros(max_l)\n",
    "    label[label1] = (1 - f)\n",
    "    label[label2] = f\n",
    "    return label\n",
    "    \n",
    "    \n",
    "\n",
    "def blend(image1, image2, label1, label2, alpha = 0.2,max_l = 48):\n",
    "    param_rand = random.betavariate(alpha, alpha)\n",
    "    blended_image = blend_images(image1, image2, param_rand)\n",
    "    blended_label = blend_labels(label1, label2 , param_rand, max_l)\n",
    "    return blended_image, blended_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd8375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d623a02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmented_dataset(dataset,nb_imgs):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(nb_imgs):\n",
    "        image_indices = random.sample(range(len(dataset)), 2)\n",
    "        image1, label1 = dataset[image_indices[0]]\n",
    "        image2, label2 = dataset[image_indices[1]]\n",
    "        blended_image, blended_label = blend(image1, image2, label1, label2)\n",
    "        images.append(blended_image)\n",
    "        labels.append(blended_label)\n",
    "    return ImageDataset(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131443ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = DataModule(\"compressed_dataset/train\", \"compressed_dataset/unlabelled\", torchvision.transforms.Compose([torchvision.transforms.Resize(size=[224, 224]), torchvision.transforms.ToTensor()]), 32, 48)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3a64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = datamodule.labeled_dataloader(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38318b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_augmented_dataset(train_set,20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6278d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3db50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(\n",
    "            val,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b827aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_train_loader = datamodule.unlabeled_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe76840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freematch_augmentation(tensor_batch):\n",
    "    # Define the augmentation transforms\n",
    "    tensor_batch = transforms.functional.adjust_contrast(tensor_batch, 1.2)  \n",
    "    #tensor_batch = transforms.functional.gaussian_blur(tensor_batch, kernel_size=(3, 3))\n",
    "\n",
    "    # Additional augmentation transforms\n",
    "    tensor_batch = transforms.functional.adjust_brightness(tensor_batch, 0.8)\n",
    "    tensor_batch = transforms.functional.affine(tensor_batch, angle=10, translate=(0.2, 0.2), scale=0.8, shear = 0.)\n",
    "    tensor_batch = transforms.functional.hflip(tensor_batch)\n",
    "    #tensor_batch = transforms.functional.normalize(tensor_batch, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "\n",
    "    return tensor_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d5be6f",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753df24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFinetune(nn.Module):\n",
    "    def __init__(self, num_classes, frozen=False):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet50(pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        if frozen:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.classifier = nn.Linear(2048, num_classes)\n",
    "        #self.load_model_weights(\"model_pretrain.pt\")\n",
    "        \n",
    "    def load_model_weights(self, model_path):\n",
    "        state_dict = torch.load(model_path)\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f33ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "class VisionFinetune(nn.Module):\n",
    "    def __init__(self, frozen = False):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.classifier = nn.Linear(1000, 48)\n",
    "        if frozen:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        #self.load_model_weights(\"model_pretrain.pt\")\n",
    "        \n",
    "    def load_model_weights(self, model_path):\n",
    "        state_dict = torch.load(model_path)\n",
    "        self.backbone.load_state_dict(state_dict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484525b6",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6006d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = wandb.init(project=\"challenge\", name=\"run_vision\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionFinetune().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_fn = loss_fn = lambda input, target: torch.nn.functional.cross_entropy(input, target, reduction='mean', label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e08e7",
   "metadata": {},
   "source": [
    "# Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db402281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(7, 8):\n",
    "\n",
    "    train_loader, val_loader = datamodule.labeled_dataloader(j//2)\n",
    "    model = VisionFinetune().to(device)\n",
    "    optimizer = torch.optim.ASGD(model.parameters(), lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0., foreach=None)\n",
    "\n",
    "    for epoch in tqdm(range(1)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_num_correct = 0\n",
    "        num_samples = 0\n",
    "        accumulated_loss = 0\n",
    "    \n",
    "            # Labeled data\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(images)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += loss.detach().cpu().numpy() * len(images)\n",
    "            epoch_num_correct += (\n",
    "                (preds.argmax(1) == labels).sum().detach().cpu().numpy()\n",
    "                )\n",
    "            num_samples += len(images)\n",
    "            \n",
    "        epoch_loss /= num_samples\n",
    "        epoch_acc = epoch_num_correct / num_samples\n",
    "        logger.log(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss_epoch\": epoch_loss,\n",
    "                    \"train_acc\": epoch_acc,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "            # Validation\n",
    "        if epoch % 1 == 0:  # Evaluate the model every 5 epochs\n",
    "            model.eval()\n",
    "            epoch_loss = 0\n",
    "            epoch_num_correct = 0\n",
    "            num_samples = 0\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(val_loader):\n",
    "                    images, labels = batch\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    preds = model(images)\n",
    "                    loss = loss_fn(preds, labels)\n",
    "                    epoch_loss += loss.item() * len(images)\n",
    "                    epoch_num_correct += (preds.argmax(1) == labels).sum().item()\n",
    "                    num_samples += len(images)\n",
    "    \n",
    "            epoch_loss /= num_samples\n",
    "            epoch_acc = epoch_num_correct / num_samples\n",
    "            logger.log(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"val_loss_epoch\": epoch_loss,\n",
    "                    \"val_acc\": epoch_acc,\n",
    "                }\n",
    "            )\n",
    "        torch.save(model.state_dict(), \"model\"+str(j)+\".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31750a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model-1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d8d19b",
   "metadata": {},
   "source": [
    "# Free Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b907c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "decay = 0.999\n",
    "t = 0.8\n",
    "\n",
    "\n",
    "grad_accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
    "accumulated_loss = 0\n",
    "num_samples = 0\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_num_correct = 0\n",
    " \n",
    "    # Labeled data\n",
    "    for batch_idx, (data_l, data_ul) in enumerate(zip(train_loader, unlabeled_train_loader)):\n",
    "        images_l, labels = data_l\n",
    "        images_l = images_l.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds_l = model(images_l)\n",
    "        Ls = loss_fn(preds_l, labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            images_ul = data_ul[0]\n",
    "            images_ul = images_ul.to(device)\n",
    "            preds_ul = model(images_ul)\n",
    "\n",
    "            condition = (torch.nn.functional.softmax(preds_ul, dim=1).max(dim=1)[0] > t)\n",
    "        \n",
    "        if len(images_ul[condition]) != 0:\n",
    "            indices = torch.argmax(preds_ul[condition], dim=1).to(device)\n",
    "            img = images_ul[condition].reshape(len(images_ul[condition]), 3, 224, 224) * 255.0  # case [0, 1]\n",
    "            img = torch.clip(img, 0.0, 255.0)\n",
    "            img = img.type(torch.uint8)\n",
    "            img = RandAugment()(img)\n",
    "            img = img.type(torch.float32) / 255.0\n",
    "            preds_aug_ul = model(img)\n",
    "        \n",
    "            Lu = loss_fn(preds_aug_ul, indices)\n",
    "\n",
    "            loss = Lu + Ls\n",
    "        else:\n",
    "            loss = Ls\n",
    "\n",
    "        # Gradient accumulation\n",
    "        accumulated_loss += loss\n",
    "        num_samples += len(images_l)\n",
    "        epoch_num_correct += (preds_l.argmax(1) == labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "            accumulated_loss /= grad_accumulation_steps\n",
    "            accumulated_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += accumulated_loss.item() * num_samples\n",
    "\n",
    "            accumulated_loss = 0\n",
    "            num_samples = 0\n",
    "\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    epoch_acc = epoch_num_correct / len(train_loader.dataset)\n",
    "    logger.log({\n",
    "        \"train_loss_epoch\": epoch_loss,\n",
    "        \"train_acc\": epoch_acc,\n",
    "    })\n",
    "\n",
    "    # Validation\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                preds = model(images)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                epoch_loss += loss.item() * len(images)\n",
    "                epoch_num_correct += (preds.argmax(1) == labels).sum().item()\n",
    "                \n",
    "\n",
    "        epoch_loss /= len(val_loader.dataset)\n",
    "        epoch_acc = epoch_num_correct / len(val_loader.dataset)\n",
    "        logger.log({\n",
    "            \"val_loss_epoch\": epoch_loss,\n",
    "            \"val_acc\": epoch_acc,\n",
    "        })\n",
    "    torch.save(model.state_dict(), \"model.pt\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b451b",
   "metadata": {},
   "source": [
    "# Fix Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d05acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "decay = 0.999\n",
    "t = 0.8\n",
    "\n",
    "\n",
    "grad_accumulation_steps = 4  # Accumulate gradients over 4 batches\n",
    "accumulated_loss = 0\n",
    "num_samples = 0\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_num_correct = 0\n",
    " \n",
    "    # Labeled data\n",
    "    for batch_idx, (data_l, data_ul) in enumerate(zip(train_loader, unlabeled_train_loader)):\n",
    "        images_l, labels = data_l\n",
    "        images_l = images_l.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds_l = model(images_l)\n",
    "        Ls = loss_fn(preds_l, labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            images_ul = data_ul[0]\n",
    "            images_ul = images_ul.to(device)\n",
    "            preds_ul = model(images_ul)\n",
    "\n",
    "            condition = (torch.nn.functional.softmax(preds_ul, dim=1).max(dim=1)[0] > t)\n",
    "        \n",
    "        if len(images_ul[condition]) != 0:\n",
    "            indices = torch.argmax(preds_ul[condition], dim=1).to(device)\n",
    "            img = images_ul[condition].reshape(len(images_ul[condition]), 3, 224, 224) * 255.0  # case [0, 1]\n",
    "            img = torch.clip(img, 0.0, 255.0)\n",
    "            img = img.type(torch.uint8)\n",
    "            img = RandAugment()(img)\n",
    "            img = img.type(torch.float32) / 255.0\n",
    "            preds_aug_ul = model(img)\n",
    "        \n",
    "            Lu = loss_fn(preds_aug_ul, indices)\n",
    "\n",
    "            loss = Lu + Ls\n",
    "        else:\n",
    "            loss = Ls\n",
    "\n",
    "        # Gradient accumulation\n",
    "        accumulated_loss += loss\n",
    "        num_samples += len(images_l)\n",
    "        epoch_num_correct += (preds_l.argmax(1) == labels).sum().item()\n",
    "\n",
    "        if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "            accumulated_loss /= grad_accumulation_steps\n",
    "            accumulated_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += accumulated_loss.item() * num_samples\n",
    "\n",
    "            accumulated_loss = 0\n",
    "            num_samples = 0\n",
    "\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    epoch_acc = epoch_num_correct / len(train_loader.dataset)\n",
    "    logger.log({\n",
    "        \"train_loss_epoch\": epoch_loss,\n",
    "        \"train_acc\": epoch_acc,\n",
    "    })\n",
    "\n",
    "    # Validation\n",
    "    if epoch % 1 == 0:\n",
    "        model.eval()\n",
    "        epoch_loss = 0\n",
    "        epoch_num_correct = 0\n",
    "        num_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_loader):\n",
    "                images, labels = batch\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                preds = model(images)\n",
    "                loss = loss_fn(preds, labels)\n",
    "                epoch_loss += loss.item() * len(images)\n",
    "                epoch_num_correct += (preds.argmax(1) == labels).sum().item()\n",
    "                \n",
    "\n",
    "        epoch_loss /= len(val_loader.dataset)\n",
    "        epoch_acc = epoch_num_correct / len(val_loader.dataset)\n",
    "        logger.log({\n",
    "            \"val_loss_epoch\": epoch_loss,\n",
    "            \"val_acc\": epoch_acc,\n",
    "        })\n",
    "    torch.save(model.state_dict(), \"model.pt\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23156b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
