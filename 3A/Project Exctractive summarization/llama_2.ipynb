{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = 'meta-llama/Llama-2-7b-hf'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_SAMPLE = True\n",
    "MAX_NEW_TOKENS = 1024\n",
    "TEMPERATURE = .3\n",
    "TOP_K = 100\n",
    "\n",
    "def result_from_output_sequence(output, n_shot = 0):\n",
    "    \n",
    "    # Un-prompt output\n",
    "    delimiter = '[/INST]'\n",
    "    output = delimiter.join(output.split(delimiter)[1 + n_shot:])\n",
    "    \n",
    "    # Remove white spaces in front of summary\n",
    "    while len(output) > 0 and output[0] == ' ':\n",
    "        output = output[1:]\n",
    "    \n",
    "    # Remove <|endoftext|>\n",
    "    eos_token = '</s>'\n",
    "    output = output[:-len(eos_token)]\n",
    "    \n",
    "    return output\n",
    "\n",
    "def generate_output(prompt, counter, chunks):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    print('---- Computing chunk', counter, 'of', len(chunks), 'total, size:', len(input_ids['input_ids'][0]), 'tokens')\n",
    "    output_ids = model.generate(**input_ids, do_sample=DO_SAMPLE, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE, top_k=TOP_K)\n",
    "    return tokenizer.decode(output_ids[0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(text):\n",
    "    return len(tokenizer(text, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "def append_to_chunk(current_chunk, utterance):\n",
    "    if len(current_chunk) > 0:\n",
    "        current_chunk += '\\n'\n",
    "    current_chunk += utterance\n",
    "    return current_chunk\n",
    "\n",
    "def chunkize(text, max_chunk_len):\n",
    "    \"\"\"\n",
    "    Greedy implementation of a dialogue transcript chunking algorithm. This method returns a list of transcript chunks.\n",
    "    - It priorities stability over performance. There is a set maximum chunk size for LLM inference stability. Really long utterances bypass this limit.\n",
    "    - It guarantees the cuts are made at utterance ends (\\n).\n",
    "    - It counts everything in MODEL TOKENS and not characters for more exact experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = [] # Final list of transcript chunks. This makes up the loop invariant\n",
    "    utterances = text.split('\\n') # Transcript is split into sentences\n",
    "    utterances.reverse() # Reverse everything!!\n",
    "    current_chunk = ''\n",
    "\n",
    "    # While there is still an utterance to process\n",
    "    while len(utterances) > 0:\n",
    "        utterance = utterances.pop()\n",
    "\n",
    "        # Add to current chunk and proceed to next\n",
    "        new_current_chunk = append_to_chunk(current_chunk, utterance)\n",
    "        if token_len(new_current_chunk) <= max_chunk_len:\n",
    "            current_chunk = new_current_chunk\n",
    "        \n",
    "        # Utterance is larger than maximum chunk size\n",
    "        elif len(current_chunk) == 0:\n",
    "            chunks.append(current_chunk)\n",
    "            chunks.append(utterance)\n",
    "            current_chunk = ''\n",
    "\n",
    "        # Current chunk is big enough, append to list and create new one\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = utterance\n",
    "\n",
    "    if len(current_chunk) > 0:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_sample(sample, max_chunk_len):\n",
    "    \n",
    "    # Current summary is the concatenation of all the chunk sub-summaries \n",
    "    current_summary = ''\n",
    "    #previous_summary = ''\n",
    "    chunks = chunkize(sample, max_chunk_len)\n",
    "\n",
    "    # Split into chunks\n",
    "    counter = 0\n",
    "    for chunk in chunks:\n",
    "        counter += 1\n",
    "\n",
    "        # Create prompt for this chunk\n",
    "        #if len(previous_summary) == 0:\n",
    "        #    prompt = PROMPT_TEMPLATE_1.format(text=chunk)\n",
    "        #    n_shot = 2\n",
    "        #else:\n",
    "        #    prompt = PROMPT_TEMPLATE_2.format(summary=previous_summary, text=chunk)\n",
    "        #    n_shot = 0\n",
    "        prompt = PROMPT_TEMPLATE.format(text=chunk)\n",
    "        #n_shot = 0\n",
    "            \n",
    "        # Sample one sub-input\n",
    "        output = generate_output(prompt, counter, chunks)\n",
    "        subsummary = result_from_output_sequence(output)\n",
    "        print(subsummary)\n",
    "\n",
    "        # Add to summary\n",
    "        if len(current_summary) > 0:\n",
    "            current_summary += '\\n\\n'\n",
    "        current_summary += subsummary\n",
    "        #previous_summary = subsummary\n",
    "    \n",
    "    return current_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "def mkdir(folder_path):\n",
    "    try:\n",
    "        os.mkdir(folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "pattern1 = r\" <.*?>\"\n",
    "pattern2 = r\"<.*?> \"\n",
    "\n",
    "def clean_utterance(utt_text):\n",
    "        utt_text = re.sub(pattern1, \"\", utt_text)\n",
    "        utt_text = re.sub(pattern2, \"\", utt_text)\n",
    "        return utt_text\n",
    "\n",
    "# Define prompt template\n",
    "# ==========================================================================================\n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an artificial intelligence assistant. You must give helpful, detailed, and polite answers to the instructed provided.\n",
    "<</SYS>>\n",
    "\n",
    "Summarize the following text.\n",
    "\n",
    "{text}[/INST]\"\"\"\n",
    "\n",
    "# Load meeting transcript\n",
    "file_path = '../training/ES2002a.json'\n",
    "meeting_file = open(file_path, 'r')\n",
    "meeting_json = json.loads(meeting_file.read())\n",
    "meeting_file.close()\n",
    "\n",
    "# Calculate text input\n",
    "text = \"\"\n",
    "for utterance_json in meeting_json:\n",
    "    text += utterance_json['speaker'] + ': ' + clean_utterance(utterance_json['text']) + '\\n'\n",
    "\n",
    "output = inference_sample(text, 3000)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
